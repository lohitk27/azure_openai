{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bdd1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import openai\n",
    "\n",
    "AOAI_endpoint = \"https://openaiykus.openai.azure.com/\"\n",
    "key_value = \"ha73n4k4n1884b5\"\n",
    "\n",
    "keyvault = ws.get_default_keyvault()\n",
    "keyvault.set_secret(name=\"aoai-endpoint\", value=AOAI_endpoint)\n",
    "keyvault.set_secret(name=\"key\", value=key_value)\n",
    "\n",
    "aoai_endpoint = keyvault.get_secret(name=\"aoai-endpoint\")\n",
    "aoai_key = keyvault.get_secret(name=\"key\")\n",
    "\n",
    "# Set up Azure OpenAI\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = aoai_endpoint  # Api base is the 'Endpoint' which can be found in Azure Portal where Azure OpenAI is created. It looks like https://xxxxxx.openai.azure.com/\n",
    "openai.api_version = \"2022-12-01\"\n",
    "openai.api_key = aoai_key\n",
    "\n",
    "prompt = \"Extract the person name, company name, location and phone number from the text below.\\n\\n Hello. My name is Robert Smith. Iâ€™m calling from Contoso Insurance, Delaware. My colleague mentioned that you are interested in learning about our comprehensive benefits policy. when you get a chance so we can go over the benefits?\"\n",
    "\n",
    "response = openai.Completion.create(deployment_id=\"textdavinci003yk\",\n",
    "                                    prompt=prompt,\n",
    "                                    temperature=0.1,\n",
    "                                    max_tokens=100,\n",
    "                                    top_p=1,\n",
    "                                    frequency_penalty=0,\n",
    "                                    presence_penalty=0,\n",
    "                                    stop=None)\n",
    "\n",
    "# print response\n",
    "response['choices'][0]['text']\n",
    "\n",
    "\n",
    "\n",
    "##################################\n",
    "! pip install openai python-dotenv\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set up Azure OpenAI\n",
    "load_dotenv(\"credentials.env\")\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\") # Api base is the 'Endpoint' which can be found in Azure Portal where Azure OpenAI is created. It looks like https://xxxxxx.openai.azure.com/\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "prompt= \"Write a job description for the following job title: 'Business Intelligence Analyst'. The job description should outline the main responsibilities of the role, list the required qualifications, highlight unique benefits like flexible working hours, and provide information on how to apply.\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  deployment_id=\"gpt-35-turbo\", \n",
    "  prompt= prompt,\n",
    "  temperature=0.1,\n",
    "  max_tokens=500,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0,\n",
    "  stop=None\n",
    ")\n",
    "\n",
    "# print response\n",
    "response['choices'][0]['text']\n",
    "\n",
    "\n",
    "########################################### TOKENS RELATED\n",
    "\n",
    "#The GPT family of models process text using tokens, which are common sequences of characters found in text. The models understand the statistical relationships between these tokens, and excel at producing the next token in a sequence of tokens.\n",
    "import os\n",
    "import openai\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\") #GPT2TokenizerFast is a class for tokenizing text using the GPT-2 model. It is based on byte-level Byte-Pair-Encoding and can encode or decode text quickly. \n",
    "\n",
    "prompt = \"The road to creating new medicines and vaccines has traditionally been long and winding!\"\n",
    "\n",
    "tokens = tokenizer(prompt)\n",
    "print('Total number of tokens:', len(tokens['input_ids']))\n",
    "print('Tokens : ', [tokenizer.decode(t) for t in tokens['input_ids']])\n",
    "print(\"Tokens' numerical values:\", tokens['input_ids'])\n",
    "#pip install tiktoken #The open source version of tiktoken can be installed from PyPI\n",
    "\n",
    "import tiktoken \n",
    "\n",
    "cl100k_base = tiktoken.get_encoding(\"cl100k_base\") \n",
    "\n",
    "enc = tiktoken.Encoding( \n",
    "    name=\"gpt-35-turbo\",  \n",
    "    pat_str=cl100k_base._pat_str, \n",
    "    mergeable_ranks=cl100k_base._mergeable_ranks, \n",
    "    special_tokens={ \n",
    "        **cl100k_base._special_tokens, \n",
    "        \"<|im_start|>\": 100264, \n",
    "        \"<|im_end|>\": 100265\n",
    "    } \n",
    ") \n",
    "\n",
    "tokens = enc.encode( \n",
    "    \"The road to creating new medicines and vaccines has traditionally been long and winding!\"\n",
    ") \n",
    "\n",
    "print('Total number of tokens:', len(tokens))\n",
    "print('Tokens : ', [enc.decode([t]) for t in tokens])\n",
    "print(\"Tokens' numerical values:\", tokens)\n",
    "\n",
    "#https://platform.openai.com/tokenizer\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    engine=\"gpt-35-turbo\",\n",
    "    prompt=prompt,\n",
    "    max_tokens=60,\n",
    "    n=2\n",
    ")\n",
    "\n",
    "print('='*30, 'ANSWER #1', '='*30)\n",
    "print(response['choices'][0]['text'])\n",
    "print('='*30, 'ANSWER #2', '='*30)\n",
    "print(response['choices'][1]['text'])\n",
    "\n",
    "response['usage']\n",
    "\n",
    "###############################################################\n",
    "# Create your first prompt\n",
    "text_prompt = \"Should oxford commas always be used?\"\n",
    "# Simple API Call\n",
    "openai.Completion.create(\n",
    "    deployment_id=\"gpt-35-turbo\",\n",
    "    prompt=text_prompt,\n",
    "    max_tokens=60\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")\n",
    "\n",
    "openai.Completion.create(\n",
    "    deployment_id=\"gpt-35-turbo\",\n",
    "    prompt=text_prompt,\n",
    "    max_tokens=60\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")\n",
    "\n",
    "\n",
    "\n",
    "prompt = f\"\"\"Classify the following inquiry into one of the following: categories: [Pricing, Hardware Support, Software Support]\\n\\n\n",
    "inquiry: Hello, one of the keys on my laptop keyboard broke recently and I'll need a replacement:\\n\\n\n",
    "Classified category:\"\"\"\n",
    "\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  deployment_id=\"gpt-35-turbo\",\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=60,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0,\n",
    "  stop=\"inquiry\")\n",
    "\n",
    "print(response)\n",
    "\n",
    "prompt = f\"\"\"Answer the question as truthfully as possible, and if you're unsure of the answer, say \"Sorry, I don't know\".\n",
    "\n",
    "Q: Who won the 2020 Summer Olympics men's high jump?\n",
    "A:\"\"\"\n",
    "\n",
    "response= openai.Completion.create(\n",
    "    deployment_id=\"gpt-35-turbo\",#textdavinci003yk\",#\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=100,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0\n",
    ")\n",
    "\n",
    "print(response)\n",
    "# Answer: Italian athlete Gianmarco Tamberi along with Qatari athlete Mutaz Essa Barshim emerged as joint winners\n",
    "\n",
    "response[\"choices\"][0][\"text\"].strip(\" \\n\")\n",
    "\n",
    "prompt = f\"\"\"Decide whether the following customer feedback is positive or negative.\n",
    "\n",
    "Q: I was disappointed with the quality of the product. It was very cheaply made and did not meet my expectations at all.\n",
    "\"\"\"\n",
    "\n",
    "openai.Completion.create(\n",
    "    deployment_id=\"gpt-35-turbo\",\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=10,\n",
    "    stop=\"\\nQ\"\n",
    "\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")\n",
    "\n",
    "prompt = f\"\"\"List all PII data from following statement:\n",
    "John Doe is a 35-year old man and he lives at 21 Main Street, New York, NY. He is a software engineer and he works at Google. He has a wife named Jane Doe and they have two children\n",
    "\"\"\"\n",
    "\n",
    "openai.Completion.create(\n",
    "    deployment_id=\"gpt-35-turbo\",\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    stop=\"\\nQ\"\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")\n",
    "\n",
    "################################\n",
    "# Chat Completions\n",
    "# Chat models take a series of messages as input, and return a model-generated message as output. The main input is the messages parameter. Messages must be an array of message objects, where each object has a role (either \"system\", \"user\", or \"assistant\") and content (the content of the message).\n",
    "\n",
    "# if needed, install and/or upgrade to the latest version of the OpenAI Python library\n",
    "%pip install --upgrade openai\n",
    "\n",
    "import openai\n",
    "# Defining a function to send the prompt to the AOAI model\n",
    "# More info : https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt?pivots=programming-language-chat-completions\n",
    "def send_message(messages, model_name, max_response_tokens=500):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        deployment_id=model_name,\n",
    "        messages=messages,\n",
    "        temperature=0.5,\n",
    "        max_tokens=max_response_tokens,\n",
    "        top_p=0.9,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Defining a function to print out the conversation in a readable format\n",
    "def print_conversation(messages):\n",
    "    for message in messages:\n",
    "        print(f\"[{message['role'].upper()}]\")\n",
    "        print(message['content'])\n",
    "        print()\n",
    "\n",
    "base_system_message = \"You are a helpful assistant.\"\n",
    "\n",
    "system_message = f\"{base_system_message.strip()}\"\n",
    "print(system_message)\n",
    "\n",
    "# This is the first user message that will be sent to the model. Feel free to update this.\n",
    "user_message = \"I want to write a blog post about the impact of AI on the future of work.\"\n",
    "\n",
    "# Create the list of messages. role can be either \"user\" or \"assistant\" \n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"name\":\"example_user\", \"content\": user_message}\n",
    "]\n",
    "\n",
    "# A sample API call for chat completions looks as follows:\n",
    "# Messages must be an array of message objects, where each object has a role (either \"system\", \"user\", or \"assistant\") and content (the content of the message).\n",
    "# For more info: https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference#chat-completions\n",
    "# You can get \"resource not found error in case your version isn't correct\"\n",
    "\n",
    "model_name= \"gpt-4-32k\"\n",
    "\n",
    "try:\n",
    "    max_response_tokens = 500\n",
    "\n",
    "    response = send_message(messages, model_name, max_response_tokens)\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    print_conversation(messages)    \n",
    "\n",
    "except openai.error.APIError as e:\n",
    "    # Handle API error here, e.g. retry or log\n",
    "    print(f\"OpenAI API returned an API Error: {e}\")\n",
    "\n",
    "except openai.error.AuthenticationError as e:\n",
    "    # Handle Authentication error here, e.g. invalid API key\n",
    "    print(f\"OpenAI API returned an Authentication Error: {e}\")\n",
    "\n",
    "except openai.error.APIConnectionError as e:\n",
    "    # Handle connection error here\n",
    "    print(f\"Failed to connect to OpenAI API: {e}\")\n",
    "\n",
    "except openai.error.InvalidRequestError as e:\n",
    "    # Handle connection error here\n",
    "    print(f\"Invalid Request Error: {e}\")\n",
    "\n",
    "except openai.error.RateLimitError as e:\n",
    "    # Handle rate limit error\n",
    "    print(f\"OpenAI API request exceeded rate limit: {e}\")\n",
    "\n",
    "except openai.error.ServiceUnavailableError as e:\n",
    "    # Handle Service Unavailable error\n",
    "    print(f\"Service Unavailable: {e}\")\n",
    "\n",
    "except openai.error.Timeout as e:\n",
    "    # Handle request timeout\n",
    "    print(f\"Request timed out: {e}\")\n",
    "\n",
    "except:\n",
    "    # Handles all other exceptions\n",
    "    print(\"An exception has occured.\")\n",
    "\n",
    "##########################################################\n",
    "# Semantic text search using Azure OpenAI embeddings\n",
    "# We can search through all our reviews semantically in a very efficient manner and at very low cost, by simply embedding our search query, and then finding the most similar reviews.\n",
    "\n",
    "#pip install openai num2words matplotlib plotly scipy scikit-learn transformers\n",
    "#!pip install --upgrade openai\n",
    "#!pip install --upgrade NumPy==1.24.0\n",
    "#!pip install --upgrade numpy==1.22.4\n",
    "#!pip install --upgrade numpy\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set up Azure OpenAI\n",
    "load_dotenv(\"credentials.env\")\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\") # Api base is the 'Endpoint' which can be found in Azure Portal where Azure OpenAI is created. It looks like https://xxxxxx.openai.azure.com/\n",
    "openai.api_version =\"2023-03-15-preview\"\n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "print(openai.api_base)\n",
    "\n",
    "\n",
    "openai.Model.list()\n",
    "#output:<OpenAIObject list at 0x7f965ada6930> JSON: {\n",
    
    "# list models deployed with embeddings capability\n",
    "deployment_id = None\n",
    "result = openai.Deployment.list()\n",
    "\n",
    "for deployment in result.data:\n",
    "    if deployment[\"status\"] != \"succeeded\":\n",
    "        continue\n",
    "\n",
    "    model = openai.Model.retrieve(deployment[\"model\"])\n",
    "    if model[\"capabilities\"][\"embeddings\"] != True:\n",
    "        continue\n",
    "\n",
    "    deployment_id = deployment[\"id\"]\n",
    "    break\n",
    "\n",
    "# if not model deployed, deploy one\n",
    "if not deployment_id:\n",
    "    print('No deployment with status: succeeded found.')\n",
    "    model = \"text-similarity-davinci-001\"\n",
    "\n",
    "    # Now let's create the deployment\n",
    "    print(f'Creating a new deployment with model: {model}')\n",
    "    result = openai.Deployment.create(model=model, scale_settings={\"scale_type\":\"standard\"})\n",
    "    deployment_id = result[\"id\"]\n",
    "    print(f'Successfully created {model} with deployment_id {deployment_id}')\n",
    "else:\n",
    "    print(f'Found a succeeded deployment that supports embeddings with id: {deployment_id}.')\n",
    "\n",
    "# Function to read paragraphs from the text file\n",
    "def read_paragraphs(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "    return paragraphs\n",
    "\n",
    "file_path = './Alice.txt' #27,000 words\n",
    "paragraphs = read_paragraphs(file_path)\n",
    "\n",
    "paragraphs_df = pd.DataFrame({'paragraph': paragraphs})\n",
    "\n",
    "paragraphs_df['paragraph'].count()\n",
    "paragraphs_df = paragraphs_df[paragraphs_df[\"paragraph\"] != ''] #Remove black strings\n",
    "paragraphs_df['embedding'] = paragraphs_df[\"paragraph\"].apply(lambda x : get_embedding(x, engine = \"text-embedding-ada-002\"))\n",
    "#4 mins v1\n",
    "#3 mins v2\n",
    "\n",
    "#https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/integrate-synapseml\n",
    "# Load pre-calculated embedding from file\n",
    "datafile_path = \"data/Alice_with_embeddings.csv\"\n",
    "df = pd.read_csv(datafile_path)\n",
    "df[\"embedding\"] = df.embedding.apply(eval).apply(np.array)\n",
    "df.head(5)\n",
    "\n",
    "\n",
    "# search through the reviews for a specific product using natural language text\n",
    "def search_reviews(df, query, n=3, pprint=True):\n",
    "\n",
    "    query_embedding = get_embedding(query,engine=\"text-embedding-ada-002\")\n",
    "\n",
    "    df[\"similarity\"] = df.embedding.apply(lambda x: cosine_similarity(x, query_embedding)) \n",
    "    results = (\n",
    "        df.sort_values(\"similarity\", ascending=False)\n",
    "        .head(n)\n",
    "        .paragraph.str.replace(\"Summary: \", \"\")\n",
    "        .str.replace(\"; Text:\", \": \")\n",
    "    )\n",
    "    if pprint:\n",
    "        for r in results:\n",
    "            print(r[:600]) #slicing till 600 index\n",
    "            print()\n",
    "    return results\n",
    "\n",
    "results = search_reviews(df, \"I like the red queen and cats?\", n=4)\n",
    "\n",
    "strPrompt = f\"\"\" Answer the question based on the context below. \n",
    "Q:Is the cat nice?\n",
    "\n",
    "### Context:\n",
    "The Cat only grinned when it saw Alice. It looked good-natured, she\n",
    "thought: still it had VERY long claws and a great many teeth, so she\n",
    "felt that it ought to be treated with respect.\n",
    "\n",
    "'How do you like the Queen?' said the Cat in a low voice.\n",
    "\n",
    "'How are you getting on?' said the Cat, as soon as there was mouth\n",
    "enough for it to speak with.\n",
    "###\n",
    "\n",
    "\"\"\"\n",
    "response = openai.Completion.create(\n",
    "    engine=\"gpt-35-turbo\",#textdavinci003yk\",#gpt-35-turbo\",\n",
    "    prompt=strPrompt,\n",
    "    temperature=0.2, #temperatue 0-1: 0 same answers 1 different answers \n",
    "    max_tokens=600,\n",
    "    frequency_penalty=0.0,\n",
    "    presence_penalty=0.0,\n",
    "    stop=\"###\"\n",
    ")\n",
    "#  top_p=1.0,\n",
    "\n",
    "text = response['choices'][0]['text'].replace(' .', '.').strip()\n",
    "print(text)\n",
    "\n",
    "strPrompt = \"\"\"Alice thought she might as well go back, and see how the game was going\n",
    "on, as she heard the Queen's voice in the distance, screaming with\n",
    "passion. She had already heard her sentence three of the players to be\n",
    "executed for having missed their turns, and she did not like the look\n",
    "of things at all, as the game was in such confusion that she never knew\n",
    "whether it was her turn or not. So she went in search of her hedgehog.\n",
    "\n",
    "Is the red queen a bad person?\n",
    "\"\"\"\n",
    "response = openai.Completion.create(\n",
    "    engine=\"textdavinci003yk\",\n",
    "    prompt=strPrompt,\n",
    "    temperature=0.2, #temperatue 0-1: 0 same answers 1 different answers \n",
    "    max_tokens=600,\n",
    "    frequency_penalty=0.0,\n",
    "    presence_penalty=0.0\n",
    ")\n",
    "#  top_p=1.0,\n",
    "\n",
    "text = response['choices'][0]['text'].replace(' .', '.').strip()\n",
    "print(text)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_embedding(text, deployment_id=deployment_id):\n",
    "    \"\"\" \n",
    "    Get embeddings for an input text from the dataframe. \n",
    "    \"\"\"\n",
    "    result = openai.Embedding.create(\n",
    "      deployment_id=deployment_id,\n",
    "      input=text\n",
    "    )\n",
    "    result = np.array(result[\"data\"][0][\"embedding\"])\n",
    "    return result\n",
    "\n",
    "def vector_similarity(x, y):\n",
    "    \"\"\"\n",
    "    Returns the similarity between two vectors.    \n",
    "    Because OpenAI Embeddings are normalized to length 1, the cosine similarity is the same as the dot product.\n",
    "    \"\"\"\n",
    "    similarity = np.dot(x, y)\n",
    "    return similarity \n",
    "\n",
    "def order_document_sections_by_query_similarity(query, contexts):\n",
    "    \"\"\"\n",
    "    Find the query embedding for the supplied query, and compare it against all of the pre-calculated articles embeddings\n",
    "    to find the most relevant articles. \n",
    "    Return the list of articles, sorted by relevance in descending order.\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "\n",
    "    document_similarities = sorted(\n",
    "        [(vector_similarity(query_embedding, doc_embedding), doc_index) for doc_index, doc_embedding in contexts.items()], \n",
    "        reverse=True)\n",
    "\n",
    "    return document_similarities\n",
    "\n",
    "def retrieve_relevant_documents(query, contexts = df['embedding']):\n",
    "# find text most similar to the query\n",
    "answers = order_document_sections_by_query_similarity(query=query, contexts=contexts)[0:3] # Set to top 3\n",
    "\n",
    "# print top 3\n",
    "for answer in answers:\n",
    "    print(f'similarity score:   {answer[0]}')\n",
    "    print(df['content'].loc[answer[1]], '\\n')\n",
    "\n",
    "return\n",
    "\n",
    "query = 'News about stock market.'\n",
    "retrieve_relevant_documents(query=query)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
